{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Neural Networks. \n",
    "\n",
    "## 1.0 Overview\n",
    "\n",
    "This lesson introduces you to the basics of neural network architecture in the form of deep forward networks. This architecture is the quintessential deep neural net architecture. In this lesson you will master the following:\n",
    "\n",
    "- Why is deep learning important and how it relates to representation, learning and inference.\n",
    "- How a basic Preceptron works.\n",
    "- How to apply different types of loss functions. \n",
    "- Understand why nonlinear activation is important and why rectified linear units are a good choice.\n",
    "- How back propagation works, and how you apply the chain rule of calculus to determine gradient. \n",
    "- Understand the architectural trade-off between depth and width in deep networks.\n",
    "\n",
    "The required reading for this lesson is from **Chapter 6: Deep Forward Networks** in GBC. Specially:  \n",
    "- 6.0\n",
    "- 6.1\n",
    "- 6.2\n",
    "- 6.2.1\n",
    "- 6.2.2\n",
    "- 6.2.2.1\n",
    "- 6.2.2.2\n",
    "- 6.2.2.3\n",
    "- 6.3\n",
    "- 6.3.1\n",
    "- 6.3.2\n",
    "- 6.4\n",
    "- 6.5\n",
    "- 6.5.1\n",
    "- 6.5.2\n",
    "- 6.5.3\n",
    "- 6.5.4,\n",
    "- 6.5.5\n",
    "- 6.5.6\n",
    "- 6.5.7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Why is deep learning important?\n",
    "\n",
    "Deep learning methods are a form of **artificial intelligence (AI)** or **machine intelligence**. More sepecifically, deep learning algorithms are a type of **machine learning**. \n",
    "\n",
    "What properties does machine intelligene require? There have been many answers to this question over the history of computing. In this case, we will take a practical view, sometimes known as **weak AI**. There are three key properties an intelligent machine must have. Deep learning algorithms are one of a few classes of algorithms that can do the following, essential to machine intelligence:\n",
    "\n",
    "1. **Representaion:** An intelligent machine must be able to represent a model of the world it interacts with in a general manner. Representation is key to intelligence. Without a good representation the best learning and inference algorithms will struggle. Whereas, good represenation can greatly facilitate learning and inference. In conventional machine learing the representation is model and a set of features. The representation is limited to what the features can provide directly. Deep learning algorithms, on the other hand, learn learn complex representations from raw features. This behavior allows deep learning algorithms to approximate complex relationships. Further, the representations learned often generalize well, up to a point. \n",
    "2. **Learning:** As you likely guessed from the very name, deep learning algorithms learn from data. Whereas, conventional machine learning is focused on inference,deep learning algorithms learn both inferencce and representations. As a result, deep leaning algorithms are more complex and therefore harder to train than conventional machine learning algorithms.  \n",
    "3. **Inference:** Any machine intelligence algorithm must be able to perform inference. The inference is the result produced given new input data. To be useful, the inferences produced by a machine intelligence algorithm must **generalize** beyond the cases used for learning or training. Good generalization requires both good represenations and learning which can deal with the complexity of diverse situations. Some deep learning algorithms can approach human levels of performance in inference tasks such as recognizing objects in images or understanding natural speach. \n",
    "\n",
    "The figure below shows a highly abstracted view of machine intelligene, showing the relationship between representation, learning and inferrence. In simple terms, the represenation is learned and then used to make inferences. Errors in the inferences can be used to improve the learning of the representation.   \n",
    "\n",
    "<img src=\"Figures/MachineIntelligence.jpg\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
    "\n",
    "<center>**Schematic for creating machine intelligence**</center>\n",
    "\n",
    "**That's it!** The entire rest of this course will focus on just these three points: represenation, learning and inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Forward propagation: The representation problem\n",
    "\n",
    "To create useful neutral network we need a **representation** that has two important properties.   \n",
    "\n",
    "First, there needs to be a way to represent complex functions of the input. Without this property, nothing is gained, since there are numerous machine learning algorithms that work with simple representations. We will spend the rest of this section exploring this problem.   \n",
    "\n",
    "Second, the representation needs to be learnable. Quite obviously, no machine intelligence representation is useful if there is not a practical algorithm to learn it. We will take up this problem in another section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Linear networks\n",
    "\n",
    "Let's start with the simplest possible network. It has inputs, and an output. The output is a **afine transformation** of the input values. We say this network performs an afine transformation since there is a bias term $b$. \n",
    "\n",
    "\n",
    "<img src=\"Figures/LinearNetwork.jpg\" alt=\"Drawing\" style=\"width:400px; height:250px\"/>\n",
    "\n",
    "<center>**Figure 2.1**\n",
    "**A simple afine network**</center>\n",
    "\n",
    "This output $y$ of this network is just:\n",
    "\n",
    "$$y = f(x) = \\sum_i w_i \\cdot x_i + b$$\n",
    "\n",
    "This network performs linear regression. Being able to perform only afine transformations, it can't do anything else. \n",
    "\n",
    "This representation is certainly learnable. However, it does not gain us anything over familiar linear regression methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The preceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To get started, let's have a look at a simple **preceptron** model. The perceptron was proposed by Rosenblatt (1962). He built on the earlier attempts at a neural network models by McCulloch and Pitts (1943) and Heeb (1949). The perceptron adds **nonliner activation** to the afine network. \n",
    "\n",
    "\n",
    "<img src=\"Figures/Preceptron.jpg\" alt=\"Drawing\" style=\"width:350px; height:250px\"/>\n",
    "<center>**Figure 2.2 Schematic of perceptron with nonlinear activation**</center>\n",
    "\n",
    "The output $y$ of the perceptron is given by the following:\n",
    "\n",
    "$$y = f(x) = \\sigma \\Big( \\sum_i w_i \\cdot x_i + b \\Big)$$\n",
    "\n",
    "The output of the network is now nonlinear, give the **activation function** $\\sigma(x)$. \n",
    "\n",
    "But, the preceptron is nothing more than a logistic regression classifier. The fact that the preceptron could only solve linearly separable problems was pointed out by Minsky and Papert (1969). The failure of the preceptron to learn an **exclusive or (XOR)** function is well known. See for example, Section 6.1 in GBC. \n",
    "\n",
    "Again, this representation is certainly learnable. However, as before, it does not gain us anything over well known logistic regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Forward networks - We're gonna need a better representation!\n",
    "\n",
    "The problem with the  perceptron is one of representations. There is no way that this simple network can represent anything but a linearly separable function. To represent more complex functions, we need a more complex network. In more technical terms we need a network with greater **model capacity**. \n",
    "\n",
    "What we need is a network with layers of **hidden nodes**. The figure below shows a simple example of a neural network with one **hidden layer** with two nodes. Since every node (including inputs) is connected to every other node we call this architecture a **fully connected neural network**.\n",
    "\n",
    "\n",
    "<img src=\"Figures/Hidden.jpg\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n",
    "<center>**Figure 2.3  \n",
    "Fully connected neural network with single hidden layer**</center>\n",
    "\n",
    "Let's walk through some aspects of these diagrams. \n",
    "\n",
    "1. The neural network is divided into three layers. The input layer, the hidden layer and the output layer. \n",
    "2. The values in the input layer are multiplied by a weight matrix, $W^1$.\n",
    "3. The nodes in the hidden layer sum their inputs and add a bias term, $b^1$. \n",
    "4. The outputs of the hidden layer nodes are multiplied by a weight vector, $W^2$.\n",
    "5. The output layer sums the inputs and adds another bias term, $b^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Neural network architectures - Finding representations\n",
    "\n",
    "The representations achievable by neural network with just a single hidden layer  are quite powerful. In fact, Cybenko (1989) showed that such a network with an infinite number of hidden units using sigmoidal activation can approximate any arbitrary function. Hornik (1991) generalized this to apply to any activation function. We call this theorem the **universal approximation theorem**.  \n",
    "\n",
    "A universal approximation theorem may see like a really exciting development; especially if you are a machine intelligence nerd. However, one must be circumspect when viewing such a result. A representation with an infinite number of nodes cannot be learned in any practical sense. Still it is comforting to know that, at least in principle, a representation can be learned for arbitrarily complex problems. \n",
    "\n",
    "While infinitely wide networks with a single layer are unrealistic, we are not limited to one dimension. In fact, depth is typically more effective at creating complex representations rather than width in neural networks. Depth is measured by the count of hidden layers stacked one on top of the other in the network. Hence, the term deep neural networks. \n",
    "\n",
    "The Figure 2.4 below shows the results of an empirical study by Goodfellow, Shlens and Szegedy (2014) of accuracy of the network vs depth. Notice that accuracy increases rapidly with depth until about 8 layers, after which the effect is reduced. \n",
    "\n",
    "\n",
    "<img src=\"Figures/Accuracy-Layers.jpg\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n",
    "<center>**Figure 2.4 Empirical results of accuracy vs. number of layers**  \n",
    "Diagram from Goodfellow et. al. 2014</center>\n",
    "\n",
    "Another view of the empirical study by Goodfellow et. al. is shown in Figure 2.5 below. In this case accuracy verses number of model parameters is compared for three different network architectures. The deeper network (11 layers) makes more  efficient use of the parameters in terms of improved accuracy. The number of parameters in a layer is approximately the total number of parameters divided by the number of layers. Notice that for the particular case tested convolutional neural networks are more efficient than fully-connected networks. We will discuss convolutional neural networks in a subsequent lesson. \n",
    "\n",
    "Of particular interest is the fact that the fully-connected network and the shallow convolutional neural network appear to be over-fitting as the test accuracy actually decreases as the number of parameters increases. We will discuss the significant problems of over-fitting in neural networks in a subsequent lesson. \n",
    "\n",
    "\n",
    "<img src=\"Figures/Accuracy-Parameters.jpg\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n",
    "<center>**Figure 2.5 Empirical results of accuracy for different network architectures**  \n",
    "Diagram from Goodfellow et. al. 2014</center>\n",
    "\n",
    "**Summary:** Deep networks tend to produce better models, with less tendency to over-fit, for a given level of complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Computational graphs\n",
    "\n",
    "There is another way to look at neural nets, computational graphs. A computational graph breaks down the steps of a complex algorithm into steps. \n",
    "\n",
    "Computational graphs provide a way to organize complex computations in an efficient manner. Widely used computational frameworks such as Tensor Flow, CNTK, and Torch all use computational graphs. Organizing computations in a graph allows these platform to minimize memory transfers. In simple terms, the platform can look ahead in the graph and organize data and computational results so as to minimize memory transfers. As a result, such platforms can be significantly faster than, say, Python Numpy. Systems like Numpy require memory transfer before each operation, which typically take more time than the actual computation. \n",
    "\n",
    "The diagram below decomposes the single hidden layer neural network discussed in the previous section into a computational graph. \n",
    "\n",
    "\n",
    "<img src=\"Figures/CompGraph1.jpg\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
    "<center>**Figure 2.6  \n",
    "Computational graph for fully connected neural network of Figure 2.3** </center>\n",
    "\n",
    "Let's walk though this graph, step by step.\n",
    "\n",
    "1. The $nX2$ weight tensor, $W^1$, is multiplied by the 1-dimensional input tensor $x \\in\\ R^n$, giving  the result $U^1 \\in\\ R^2$.\n",
    "2. The 1-dimensional bias tensor $b^1 \\in\\ R^2$ is added to $U^1$, giving $U^2 \\in\\ R^2$. \n",
    "2. The activation function $\\sigma_h(x)$ is applied to $U^2$, producing $U^3 \\in\\ R^2$\n",
    "3. The dot product between the weight tensor, $W^2 \\in\\ R^2$ and $U^3$ is computed giving $U^4 \\in\\ R^1$. \n",
    "4. The bias, $b^2 \\in\\ R^1$ is added to $U^4$ giving $U^5 \\in\\ R^1$.\n",
    "5. The output activation function $\\sigma_o(x)$ is applied to $U^4$ giving the output $Y \\in\\ R^1$.\n",
    "\n",
    "As you can see, the computational graph provides a complete specification for the single hidden layer neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.6 Activation functions\n",
    "\n",
    "Without a nonlinear activation function, a neural net is just an afine transformation. Afine transformations limit representation to only linearly separable functions. To create more general representations **nonlinear activation functions** are required. \n",
    "\n",
    "In present practice, four types of activation functions are generally used for fully connected networks. \n",
    "\n",
    "1. **Linear** activation is used for the output layer of regression neural networks. \n",
    "2. The **rectilinear** activation function is used for most hidden units. The rectilinear activation function is often referred to as **ReLU**.\n",
    "3. A **leaky rectilinear** activation acts like a ReLU function for positive inputs, but has a small negative bias or leakage for negative input values. The leaky ReLU activation function can improve training for some deep neural networks. \n",
    "3. The **logistic** or **sigmoid** activation function is used for binary classifiers.\n",
    "4. The **softmax** activation function is used for multi-class classifiers. \n",
    "\n",
    "Rectilinear functions are typically used as the activation function for hidden units in neural networks. The rectilinear function is defined at:\n",
    "\n",
    "$$f(x) = max(0, x)$$\n",
    "\n",
    "The rectilinear function is linear for positive responses and zero for responses less than 0.0. Notice that the derivatives of the rectilinear function are not continuous. While this might seem to be a problem, in practice, even gradient-based optimization functions work well with this activation function. \n",
    "\n",
    "The rectilinear function is plotted in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "## Compute the lectilinear function\n",
    "def reclu(x): return(max(0,x))\n",
    "x = [float(x)/100 for x in range(-100, 100)]\n",
    "y = [reclu(y) for y in x]\n",
    "## Plot the result\n",
    "def plot_figs(x,y,title, figsize = (4, 3)):\n",
    "    plt.figure(figsize=figsize).gca() # define axis\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    plt.plot(x, y)\n",
    "    plt.ylim((-0.1,1.0))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "plot_figs(x,y,'The Rectilinear Function')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another widely used activation function is the **logistic** or **sigmoid**. The sigmoid is used as the activation for the output layer of a binary classifier. The general sigmoid function can be written as:\n",
    "\n",
    "$$\\sigma(x) = \\frac{L}{1 + e^{-k(x_0-x)}}\\\\\n",
    "where\\\\\n",
    "L = max\\ value\\\\\n",
    "k = slope\\\\\n",
    "x_0 = sigmoid\\ midpoint$$\n",
    "\n",
    "With $L=1$, $k=1$, and $x_0 = 0$, the logistic function becomes: \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{1+e^x}$$\n",
    "\n",
    "The sigmoid function can asymptotically approach $0$ or $1$, but will never reach these extreme values. However, because of the rapid decrease in the derivative away from $0$ the sigmoid can **saturate** when using gradient-based training. For this reason, the sigmoid is typically not used for hidden layers in neural networks.   \n",
    "\n",
    "When used in a the binary classifier a threshold is set to determine if the result is $0$ or $1$. The threshold can be adjusted to bias the result as desired. \n",
    "\n",
    "The code in the cell below plots the sigmoid function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAADZCAYAAAC6uVKWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHllJREFUeJzt3Xl8XXWd//HXXbK1SZouKS0t0ALtp5SlgEVaoIDKrigqLoA44jYoDGof6KCC2+DgjFb98VNAio7O6AyDS7XKVhAQuiE7lNJPCSUtXdKWpmmSZr3L/HFv4RKSkLQ999zkvp+PRx7JuefmnHdvk3e+59yzRNLpNCIixS4adgARkUKgMhQRQWUoIgKoDEVEAJWhiAigMhQRASAedgAJlpndCJyanZwJvAy0Z6fnAm1Arbu/upfLnwKscvfKfYy6Z3nvBc5w96v6ec67gRPd/RsDeX4veV8Cnusxa7G7f2MvY+/3nJJ/KsNhLveXz8zqgUvc/fGcx0JI1Td3XwwsfounnQCMGcTze2p392P3It5g7WtOySOVoQB828zmAGOB77v7TwHM7FPA58nsTtkBXOnuawa6UDMbBfwUOBZIA3cDX3P3hJmdB/wbkASeBs4ATgFOBy509/eY2QeAa4FU9nlfBjqBy4GYme0CXsx5/gTgFmBG9ntucfcbB5F3Cjmj3NxpM/sE8P7scqeRGVH/g7u/0Nt6gUf7yTkZuBmYAkSAX7n797Pr+ytwF3AiMBr4irsvGui/Qfae9hkKwDp3fxuZX/YFZlZiZqcB/wDMc/fjgH8HBvtLeSOZEj0amA3MAq42s7HAfwEfy47QHgQm9fL93wc+7+6zgeuA0939UTJl87/u/vUez78JWOvuM8jsAvismR3ey3IrzOzpnI/He3lOb04D/sndjyJTdtf0td7sv7uvnL8BHnT3o4GTgY+Z2Uez8w4F7nX3t2eX/+MBZpN9pJGhAPx39vPTQBlQDbwbOBxYnrMpPdrMxrh74wCXey5wsrungU4zuwX4IuDAand/BsDdf5Xdt9nT7cAiM7sTuI9MIffnDOAr2WXuAo7q43l7u5n8hLtvzH79JPCB/tbb2y4IMxtJpgDP2vN8M/slmddqJdBNZmS4Zx1j9iKn7AWNDAUyv4BkSwsym24x4L/c/dhscRxPZnS3cxDLjZLZPM6dLgES2XXkSvX85uyI6hTgceATwMNvsb5E7vrM7FAzqx5E3nSPXKU95rf38dzBrDfKm//te14XgC533/Na9MwjAVIZSl/uBS4ys4nZ6cvJ7M8a7DKuNLOImZWR2Xy8D1gGTDezYwDM7INADW8slHj2DZ8R7n4LmX2Xx2SXk+D18sh1P3BZ9vtHZfNOG0TeJqDUzGZmpy8a4Pf1td435XT3FjIjwCtynv9xMq+LhEhlKL1y9yVk3uC4z8yeBS4GPpAzesw10sxae3wcDVwFjCdzGMtzZDaPv5vdzL4I+E8zexI4m0xxtOWsP0Fmk/q/s8/5LfBJd+8EHgDONrP/3yPHlcAR2bzLgBvc/YlB/Jt3kdncvdvMHuONI8H+9LXevnJeArzLzJ4D/g78AfjlQHNKMCK6hJfkW3YT8lrgW+7eZmbHA3cCB/ZRtiKBUxlKKMzsejLvXndnP+a7+yPhppJipjIUESHAfYZmdqKZPdTL4+eb2WNmtsLMPhPU+kVEBiOQMjSzrwC3AeU9Hi8BfkTmGKvTyBwUOyGIDCIigxHUyPAlXj8gNdcRQJ2773T3LmApMC+gDCIiAxbIGSju/vvseZY9VQO7cqZbgFG9LaO9vSsdj8cCSNe/WCxCMllY+1ELLZPy9K/Q8sDAMqXTadq6kjR3dNPU1s2u9tc/mtq7aelI0NaVpL07SVtngrbuJO1dSdq6k7R1JmnvTtDenaIrkaI7maIrmSLotyS+9K5pfP70wwb1PSUlsV4PZM/36XjNQFXOdBWZA13fpLW1My+BeqqpGUFTU9tbPzGPCi2T8vSv0PIAjBpVQd2mJjY1dbC5uYNtLZ28uruLV3d3sb0183nH7i46E286Eeg1sQhUlMaoKMl8lMejjCiNMaIkxtiKEipKolSUxCiJRSmNRYjHopREI5TGosRjkdceL4lFGVVVTndHN/FYhFg0QiyS+RyNRIhGeO3rWCRCNErO12+cP76ydNCvdW1tVa+P57sMXwCmmdkYoJXMdfZ+kOcMIsNWIpmifmc7ddt38+L23dQ3trGxqZ3NzR10dL+x6EaWxqitLGXcyFKOObCacSNLGV1RQnV5PPvxxq8rSqJEIvvn7MBC/IORlzI0s4uBSne/1czmkzlNKwr8wt035SODyHCTTqdZv7OdZzc18+zmZlZvbeHlHW0kUplt03g0wkGjKziopoLTrJZx5SVMriln0qhyxleVUVGS/91QhaxgjzPcvr0llGCF+Ber0DIpT/+CzLOluYMVLzeyon4nT23cxa6OBADV5XGOnFDFtNpKptWO5PDakUwZXUE8Fg08094IM09tbVVB7DMUkUFIp9O8sLWV+3w7S9ftoL4xc7r0xOoyTj1sLLMmVXPMgaM4ZEwF0f20CVusVIYiBWh9Yxt3rd7KEt/OxqYO4tEIbztoFO8/ZiInTRnDIWMq9tv+O8lQGYoUiEQyxcMv7eB3z2zhsQ1NRCNwwsE1XPb2gzl92liqy3u7apnsLypDkZB1dCdZ9FwDv37sFba1djGhqozPnTyF9x51AOMqy8KOVzRUhiIhae9OcsdTm/nN4xvZ2d7N8ZNH8c9nTOPkqWOIRbUJnG8qQ5E8S6XT3Pn8Vm5eVs/21i7mThnNJ088mGMn93oyluSJylAkj1ZtaeZ799fh21o5ckIVN7znCGZNUgkWApWhSB60dye5eWk9tz+5idrKUq4/bwZnzqjV4TAFRGUoErAnNzbx7budzc2dXDhrIlfMm0plmX71Co3+R0QCkkyl+Y9HN7BwxXomjSrn1o/M4jjtFyxYKkORADS2dfH1O9fw+IYmzjliPNeccTgjS/XrVsj0vyOyn3lDC5/+9VPsbO/murOnc/6RB+hskSFAZSiyHy1dt4Nr71zDiNIYt35kFjMn9H7tPCk8KkOR/eSPz27hhvtf5IiJ1fz7e45gfJXOHhlKVIYi+8F/P7GRHz20jrlTRnPLpW+jq60r7EgySIHdKlSkGKTTaRauWM+PHlrHO6eNY8EFRzJCb5QMSfpfE9kH//HoK9y6fD3vPvIArj1rOnGdUzxkqQxF9tLtT27i5mX1nDdzPN84e7rOJhnitJksshf+vKqBBQ++xOmHj+W6s01FOAyoDEUGaUV9I9cvWcuJh9Tw3XcfoU3jYUJlKDIIL726m6/++QUOGzeSf3vvTErj+hUaLgLZZ2hmUeAmYBbQCXza3ety5l8NXASkgH9190VB5BDZnxrbupi/aBXlJTF+eMGROr1umAnqz9oFQLm7zwWuARbsmWFmNcBVwFzgLODHAWUQ2W+6kym+/KfV7GjrZsEFRzKhujzsSLKfBVWGpwD3ALj7SmB2zrzdwHpgZPYjFVAGkf3mxodf5tnNzXzj7OkcqVPshqWgyrAa2JUznTSz3G2KV4DVwJPAjQFlENkv7vft3P7kJj56/CTOmjE+7DgSkKB2ejQDuX8+o+6eyH59LjARmJqdvtfMlrn733MXUFlZRjweCyhe32KxKDU1I/K+3v4UWqZiyrNueyvXL1nLcQfVcN35Rw7oDZNCe32g8DIVWh4IrgyXAecDd5jZHOC5nHk7gXag093TZtYE1PRcQGtrZ0DR+ldTM4KmprZQ1t2XQstULHk6Eymu/J+nKIlF+c4502lr7WAgaym01wcKL1OYeWpre9/NEVQZLgLONLPlQAS4zMzmA3XuvtjMzgBWmlkKWArcF1AOkb12y7J6Xty+mx+//yi9YVIEAilDd08Bl/d4eE3O/G8C3wxi3SL7wxOvNPGbxzfywVkTOfnQMWHHkTzQEaMiPbR2JvjW3c5Boyv4wmmHhh1H8kRHjYr08IMH6tje2sltFx1LRUn+38STcGhkKJJj2cuN3Ll6G5848WCOmlgddhzJI5WhSFZbV5Lv3fciU8eM4JMnHhx2HMkzlaFI1i3L6mlo6eTrZ03TBRiKkP7HRYDnG1r436c28cFZE5k1STd6L0YqQyl6iWSK7y5Zy9iRpVw5b+pbf4MMSypDKXp3PL2ZF7fv5svvPJzKMh1gUaxUhlLUGtu6uHX5euZOGc3ph48NO46ESGUoRe2mR+rpSKSY/47DiOg+JkVNZShF6/mGFhavauCi4ycxZUxhXUFF8k9lKEUplU7zgwfqGDOylE/N0TGFojKUInXX6q2s2tLCP82bqjdNBFAZShHq6E5y09J6jpxQxbkzdeVqyVAZStH5nyc3sb21iy+cdqhu/i6vURlKUWlq6+ZXf3+FUw8by3GTdaaJvE5lKEXl549uoL07yRXzpoQdRQqMylCKxsamdn739Gbee9QEDh07Muw4UmBUhlI0bl5aTywa4bMnHRJ2FClAKkMpCqsbWlji27lk9mRqK8vCjiMFSGUoReHmpfXUVJRw6ezJYUeRAqUylGHv2c3NrFy/k4+fMFkHWEufAvnJMLMocBMwC+gEPu3udTnzz+X1W4U+CVzh7ukgsogsXLGe0RUlXHjsgWFHkQIW1MjwAqDc3ecC1wAL9swwsyrg+8B73H0OUA+MCyiHFLlnNzezsn4nl54wWXe6k34FVYanAPcAuPtKYHbOvJOA54AFZvYIsNXdtweUQ4rcwhXrqdGoUAYgqDKsBnblTCfNbM8m+TjgHcA/A+cCXzSz6QHlkCL23J5R4WyNCuWtBbU3uRmoypmOunsi+/UO4DF3bwAws4eBY4G1uQuorCwjHs//D3AsFqWmprCubVdomYZKnl8uXs3oESV86rTDGJnHN04K7fWBwstUaHkguDJcBpwP3GFmc8hsFu/xBHCUmY0DmoA5wMKeC2ht7QwoWv9qakbQ1NQWyrr7UmiZhkKeVVuaefjFV7ly3lS627toau8KNU/YCi1TmHlqa6t6fTyoMlwEnGlmy4EIcJmZzQfq3H2xmX0VuDf73DvcfVVAOaRILVyxnlHlcT6kfYUyQIGUobungMt7PLwmZ/7twO1BrFvk+S3NLH95J1fOm8qIUu0rlIHRQdcy7CxcsUGjQhk0laEMK89vaWbZy418bPZkjQplUFSGMqzctjI7KjxOo0IZHJWhDBvPN7SwdF0jl8yezMhSnYMsg6MylGHjtuw7yB/WqFD2gspQhoXVGhXKPlIZyrCg4wplX6kMZch7btMulq5r5OK36XqFsvdUhjLk/eTBOqq1r1D2kcpQhrQXtrbwgG/nEo0KZR+pDGVIu23FBkZVlGhUKPtMZShD1pqtLTz80g4uO2mKRoWyz1SGMmQtXLGB6vI4H59zcNhRZBhQGcqQ5FtbefilHVx0/CSqykvCjiPDQJ9laGZH5zOIyGAsXLGeqrI4Hz1+UthRZJjob2T4WzP7Yt6SiAyQb23lby/t4KK3TdK+Qtlv+ivDtwFmZvea2YR8BRJ5K7etzI4Kj9OoUPafPv+suvtu4HNmdiqwzMwezZl3cT7CifTk21p5qG4Hnz3pEKrKNSqU/affnyYzmwHcADwE/Gc+Aon0Z+Hy9VSWxTQqlP2uzzI0s38mcx+TK939zvxFEundmq0t/O0ljQolGP39RM0GZrv7jnyFEenPrcsz+wov0jvIEoD+9hl+KJ9BRPqzuqGFR9Y18rmTdbaJBCOQnyoziwI3AbOATuDT7l7Xy3PuBP7k7rcEkUOGj1uX6yrWEqygzkC5ACh397nANcCCXp5zPTAmoPXLMLIqe8e7S2bryjQSnKDK8BTgHgB3X0lm/+NrzOxCIAXcHdD6ZRjRqFDyIag/s9XArpzppJnF3T1hZkcBFwMXAt/oawGVlWXE4/m/720sFqWmZkTe19ufQsuUzzxPbdjJivqdXH3mdCaNrw49z0AUWh4ovEyFlgeCK8NmoCpnOuruiezXHwcmAQ8AU4AuM6t393tyF9Da2hlQtP7V1IygqaktlHX3pdAy5TPPD5espaaihPNn1Pa5zmJ+fQaq0DKFmae2tqrXx4Mqw2XA+cAdZjYHeG7PDHf/yp6vzexbQEPPIhQBeGbTLlau38lVp05lRGn+txKkuARVhouAM81sORABLjOz+UCduy8OaJ0yjKTTaX7yyMuMHVnKhbrjneRBIGXo7ikyZ6/kWtPL874VxPpl6Fv2ciNPb2rmmjMOp6JEo0IJni7uKgUnlU7z00fqmVxTzvuO0gWTJD9UhlJw7nlhG3Wv7uZzJ08hHtOPqOSHftKkoHQnU/xsWT02vpIzrDbsOFJEVIZSUBY9u4XNzZ1cMW8K0Ugk7DhSRFSGUjB2dyX4+coNzD5oFHMOGR12HCkyKkMpGL/6+ys0tnVz5bypRDQqlDxTGUpB2Lyrg988vpFzjxjPkRN7P+1OJEgqQykIP3nkZSKRCFfMmxp2FClSKkMJ3TObdnGfb+fjJ0zmgKqysONIkVIZSqhS6TQ/fGgd4ytLufSEg8KOI0VMZSihunv1NlY3tHDFvKk67U5CpTKU0DR3dHPjw+s4amIV5xwxPuw4UuRUhhKam5bW09TezTXvmqYDrCV0KkMJxfNbmvnDM1v48HGTsAMqw44jojKU/Euk0txwfx3jKkv5x5MOCTuOCKAylBD87unN+LZW5p9+mO52JwVDZSh5tXlXBzcvrWfOlNG8a/q4sOOIvEZlKHmTSqf5lyVriUTg62dO0/nHUlBUhpI3f3hmC49vaOKq0w5lQnV52HFE3kBlKHmxaVc7Nz68jhMPqeH9R+tS/lJ4VIYSuGQqzb/cu5ZoJMK1Z03X5rEUpEDeyjOzKHATMAvoBD7t7nU5878EfDQ7eZe7fzuIHFIY/vOxV3jilV1cd9Z0bR5LwQpqZHgBUO7uc4FrgAV7ZpjZocAlwEnAXOAsMzsmoBwSsmc27eJny+o502o5/6gDwo4j0qegyvAU4B4Ad18JzM6Z9wpwjrsns/dXLgE6AsohIWrpSHDdXWs4oLqcr+ndYylwQR3xWg3syplOmlnc3RPu3g28amYR4PvAU+6+NqAcEpJ09jCaba1d/Pyjs3RwtRS8oH5Cm4GqnOmouyf2TJhZOfALoAX4fG8LqKwsIx7P/yWdYrEoNTUj8r7e/hRapoHkueVvL/Hgi69yzTnGyUcE++7xUHx98q3QMhVaHgiuDJcB5wN3mNkc4Lk9M7Ijwj8BD7j7v/W1gNbWzoCi9a+mZgRNTW2hrLsvhZbprfIsW9fID+9/kbNn1PKBmeMDzz7UXp8wFFqmMPPU1lb1+nhQZbgIONPMlgMR4DIzmw/UATHgNKDMzM7NPv+r7r4ioCySR+sb27j2rheYPr5Sh9HIkBJIGWbfGLm8x8Nrcr7W8RXDUFNbN/P/+DzxaJTvv28m5bpytQwh2qst+0VHd5Iv/XEVW1s6+emFRzNRxxPKEKMzUGSfJVJpvvaXF1jd0ML1581g1qRRYUcSGTSVoeyTZCrN9fc6j6xr5MvvPJzTp+myXDI0qQxlr6XSaa5fspY7V2/jcydP4cJjDww7ksheUxnKXkml0/zrfS/yl+e38tm5h/DJOQeHHUlkn+gNFBm0zkSKr/9lDfev3c6n5hzMZ3QfExkGVIYyKK2dCa5a9Dgr1jVy1alTufSEg8KOJLJfqAxlwBqaO7j6T6t56dXdfPtc47yZugqNDB8qQxmQJ15p4qt/foGuZIpbLjmeWeNHhh1JZL/SGyjSr3Q6ze1PbuKK3z1HdXmcX158HKdNrw07lsh+p5Gh9GnH7i6uX7KWpesamXfoGL5z3gxdikuGLf1kS6/+VreD7y5Zy+6uBFe/4zA+dNyBRHXRBRnGVIbyBltbOlnwYOZahNNqR3Lzecdw2DjtH5ThT2UoQObYwTue2sRtKzaQTKe54pQpXDJ7MiUx7VaW4qAyLHLJVJq7Vm/l1uXraWjp5JRDx3D1Ow9j0qiKsKOJ5JXKsEh1JVLcs2Yb//XYK9Q3tjNzQhXfOGc6Jxw8OuxoIqFQGRaZnW1dLF61lduf3MSru7uYVjuS751/BO+cNk5XpZaipjIsAslUmkfX72Txqgb+VreDRCrN2w+u4ZvnTOfEQ0arBEVQGQ5biVSapzY28eCLO3io7lW2t3ZRU1HCh487kPOPmsDheodY5A1UhsNIQ3MHj21o4rENTSx/uZFdHQnK4lFOmjqGs2fUcuphY/XusEgfVIZDVCKVZt2ru3m+oYXVDS08uXEXG3a2AzC6ooS5U8fwjmnjOGnKaN2YSWQAVIYFLp1O09DcwTPrd1K/o42XG9uo276bNdta6UykAKguj3PMgdV8cNZETji4hsPGjdTZIiKDFEgZmlkUuAmYBXQCn3b3upz5nwH+EUgA17v7X4LIMRQkkima2rvZ1tpFQ0snDc0dNDR3sqW5g60tnWzY2c7uruRrz68qi3Po2BF84JiJHDmhipkTqphcU643QUT2UVAjwwuAcnefa2ZzgAXA+wDMbAJwFTCbzP2Tl5rZfe7eGVCWQKXSaboSKdq6k+zuTLK7K8HurmT2I0Fb1+uPN7UnaGzroqm9m8a2bprau2nuSLxpmRUlUSZUlzOhqoyjJ1Yzc3INB1TEmTJ2BGNHlKj4RAIQVBmeAtwD4O4rzWx2zry3A8uy5ddpZnXAMcBje7uy1s4ES3w7XYkUqXSaZCr7kU6TSkEy+1gqnSaRSpNKQyo7f8/jmedDLBalvTNBVzJFVyJFdzJFZzJTeF3JFJ2J1GtfdyVTdCfTA8oYjUBNRQk1FSWMHlHC9NpKRo/IfD26ooRxI0uZWF3OhOoyqsvjbyi8mpoRNDW17e3LIyIDEFQZVgO7cqaTZhZ390Qv81qAN91ot7KyjHh8YDv+V6xq4Ib7Xuz3OfFohGg0kvkcibw2HYtEiEUjxKJkHo9FiUagLB6jLB6lrDROdTxKaTxGaTzy+uPZj9J4lLJ4jJFlMSrL4lSWxRmZ/fz6dIyKkthej+hisSg1NSP26nuDoDz9K7Q8UHiZCi0PBFeGzUBVznQ0W4S9zasCmnouoLV14FvNcydXc//n55KG18ouFo0Qi0A0Oz1Q+30UlkzS1Zakax8WUWgjQ+XpX6HlgcLLFGae2tqqXh8PqgyXAecDd2T3GT6XM+/vwHfNrBwoA44AVu3rCkdVlOzrIkSkiAVVhouAM81sORABLjOz+UCduy82sxuBR8jcduDr7t4RUA4RkQEJpAzdPQVc3uPhNTnzFwILg1i3iMje0LlZIiKoDEVEAJWhiAigMhQRAVSGIiKAylBEBFAZiogAKkMREUBlKCICqAxFRACIpNMDux6fiMhwppGhiAgqQxERQGUoIgLoVqFvYmYx4IdkblhVBnyrEO7eZ2YzgEeBA8K+/qOZjQJ+TeYWDqXAfHdfEUKOfu/CGEKeEuAXwBQyPzvXu/visPLsYWbjgSeAM919zVs9Pw95vgq8l8zPzk3u/vOQIwEaGfbmUqDE3U8mc0e/w0POg5lVk7nDYKHcQXA+8Fd3Pw34BPDTkHK8dhdG4Boyr1GYPgbscPd5wLnAT0LOs6egfwa0h50FwMxOB04CTgZOAw4KNVAOleGbnQ1sNLM7yVyA9s9hhjGzCHAr8DWgUG5i8SMyv2CQ2boIa6T6hrswkhnNh+m3wHU502++D2z+/QC4BdgcdpCss8ncBmQRmd+t0Le69ijqzWQz+xTwpR4Pbyfzy/0e4FTgP7Kfw8qzHrjd3Z8xs3zEGEimy9z9sew9sH8NfDHvwTL6uwtj3rl7K4CZVQG/A64NI8ceZvYJYLu735vdNC0E44BDyPx+TQUWm9kMdw/9GD8dZ9iDmd0O/Nbdf5+dbnD3CSHmqQM2ZifnAH9397yUc3/M7GjgduBqd787pAw/BFa6+x3Z6Y3uPjmMLDmZDiIz6rnJ3X8RcpaHgXT241hgLfBed28IMdP3yBT0guz0M2T2ZW4LK9MeRT0y7MNS4Dzg92Y2C9gQZhh3f22fpZnVA2eFFibLzGaS2ST8iLs/E2KU/u7CmHdmdgCwBLjS3f8aZhaA3D+aZvYQcHmYRZi1FPhC9g/ZRGAksCPcSBkqwzdbCNxsZivJ3Nmv542tBG4AyoH/l9103+Xu7wshx5vuwhhChlxfA0YD15nZnn2H57p7Qbx5UQjc/S9mdiqZWwZHgSvcPRlyLECbySIigN5NFhEBVIYiIoDKUEQEUBmKiAAqQxERQIfWyBBlZmeSOdVsjru3m9mBwL3AOe6+Kdx0MhTp0BoZsszs34FK4AvAA8B33P2+cFPJUKUylCEre0WWpWTOYHjU3b8dciQZwrTPUIYsd+8mc8bQmWQuqCGy1zQylCHLzA4hcy7wLcD7gXcUyqldMvRoZChDkpmVAncAX3L3H5G5oMY3w00lQ5nKUIaqBcBSd78rO/154KLslZRFBk2bySIiaGQoIgKoDEVEAJWhiAigMhQRAVSGIiKAylBEBFAZiogAKkMREQD+Dy31h0jBLafsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from math import exp\n",
    "def sigmoid(x): return exp(x)/(1 + exp(x))\n",
    "x = [float(x)/100 for x in range(-700, 700)]\n",
    "y = [sigmoid(y) for y in x]\n",
    "plot_figs(x,y,'The Logistic Function', figsize = (5,3))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The **softmax** function or **normalized exponential function** is used for the output activation function of a multi-class classifiers. The softmax function is the multinomial generalization of the sigmoid or logistic function. The probability of each class $j$ is written as: \n",
    "\n",
    "$$\\sigma(z_j) = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$$\n",
    "\n",
    "The normalization $\\sum_{k=1}^K e^{z_k}$ ensures the sum of probabilities for all classes add to $1.0$. The class selected by the classifier is the class with the largest value of $\\sigma(z_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Computational example\n",
    "\n",
    "Now that we have gone though some basic theory for feed-forward networks, let's try a simple example. We will construct a fully connected network to compute this simple function:\n",
    "\n",
    "$$y = x_1 - x_2$$\n",
    "\n",
    "****\n",
    "**Comment.** You have likely have noticed that this function is linear and can be computed easily without a neural network. Of course, that is not the point. We use a simple function to make the results easy to understand. \n",
    "****\n",
    "\n",
    "****\n",
    "**Note.** The neural network for this example does not require any bias terms. \n",
    "****\n",
    "\n",
    "As a first step, we will create test data for 3 cases; $x_1 > x_2$, $x_1 = x_2$, and $x_1 <x_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [(2,1), (1,1), (1,2)]\n",
    "for x_in in x:\n",
    "    print(x_in[0] - x_in[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our test data and test cases we can move to the next step. We will create the first weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "W_1 = np.array([[1.0, -1.0], [-1.0, 1.0]])\n",
    "print(W_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the weight vector, for the second set of weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_2 = np.array([1, -1])\n",
    "print(W_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the input cases and the network weights. \n",
    "\n",
    "Now, it is time to compute the results and check them. To create the computational process we will follow the graph in Figure 2.6. This particular case is simplified as there are no bias terms. \n",
    "\n",
    "The code in the cell below performs the feed forward network computation. The first function computes the matrix product of the weights with the input vector and applies a rectilinear activation function. This function computes the output of the hidden layer given the input vector. \n",
    "\n",
    "$$h = \\sigma(W^1 \\cdot x)$$\n",
    "\n",
    "The second function computes the vector product of the weight vector with the output vector of the hidden layer which is the output of the network: \n",
    "\n",
    "$$o = W^2 \\cdot h$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden(x, W):\n",
    "    \"\"\"Computes the output of the hidden layer\"\"\"\n",
    "    h = np.dot(W, x) # product of weights and input vector\n",
    "    return np.array([reclu(x) for x in h]) # apply activation function and return\n",
    "\n",
    "def output(h, W):\n",
    "    \"\"\"Computes the result for the hidden layer\"\"\"\n",
    "    return np.dot(W, h) # dot product of weight vector and input vector\n",
    "    \n",
    "## Run the test cases and check the results     \n",
    "for y in x:\n",
    "        h = hidden(y, W_1)\n",
    "        print(output(h, W_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first fully connected neural network passed all the tests!\n",
    "\n",
    "Notice that even a network to compute a simple function requires 6 weights. You can see that for more complex functions any practical algorithm must learn a large number of weights. The limitations of Numpy would quickly become evident for large scale problems involving hundreds of millions of weights. \n",
    "\n",
    "****\n",
    "**Note:** If you are having difficulty following the Numpy code in the above example, you might want to look at [Scott Shell's Numpy Tutorial](https://engineering.ucsb.edu/~shell/che210d/numpy.pdf)\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Learning in neural networks: Backpropagation\n",
    "\n",
    "Now that we have a promising representation, we need to determine if it is trainable. The answer is not only yes we can, but that we can do so in a computationally efficient manner, using a cleaver algorithm known as **backpropagation**. \n",
    "\n",
    "The backpropagation algorithm was developed independently multiple times. The earliest work on this algorithm was by Kelly (1960) in the context of control theory and Bryson (1961) in the context of dynamic programming. Rumelhart, Hinton and Williams (1984) demonstrated empirically that backpropagation can be used to train neural networks. Their paper marks the modern history of neural networks, and set off the first wave of enthusiasm. \n",
    "\n",
    "The backpropagation algorithm requires several components. First, we need a **loss function** to measure how well our representation matches the function we are trying to learn. Second, we need a way to propagate changes in the representation through the complex network For this we will use the **chain rule of calculus** to compute **gradients** of the representation. In the general case, this process requires using automatic differentiation methods. \n",
    "\n",
    "The point of backpropagration is to learn the optimal weight for the neural network. The algorithm proceeds iteratively through a series of small steps. Once we have the gradient of the loss function we can update the tensor of weights.\n",
    "\n",
    "$$W_{t+1} = W_t + \\alpha \\nabla_{W} J(W_t) $$  \n",
    "where  \n",
    "$W_t = $ the tensor of weights or model parameters at step $t$.   \n",
    "$\\alpha\\ = $ step size or learning rate.  \n",
    "$J(W) = $ loss function given the weights.  \n",
    "$\\nabla_{W} J(W) = $ gradient of $J$ with respect to the weights $W$.  \n",
    "\n",
    "It should be evident that the back propagation algorithm is a form of gradient decent. The weights are updated in small steps following the gradient of $J(W)$ down hill. \n",
    "\n",
    "Finally, we need a way evaluate the performance of the model. Without evaluation metrics we have no way to compare the performance of a given model, or compare the performance of severa models. \n",
    "\n",
    "In the next sections, we will address each of loss functions, gradient computation and performance measurement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Loss functions\n",
    "\n",
    "To train a neural network we must have a **loss function**, also known as a **cost function**. In simple terms, the loss function measures the fit of a model to the training data. The lower the loss, the better the fit. \n",
    "\n",
    "To train deep learning models **cross entropy** is often used as a loss function. This is an information theoretic measure of model fit. We can understand cross entropy as follows. \n",
    "\n",
    "First define **Shannon entropy** as:\n",
    "\n",
    "$$\\mathbb{H}(I) = E[I(X)] = E[-ln_b(P(X))] = - \\sum_{i=1}^n P(x_i) ln_b(P(x_i)$$  \n",
    "Where:  \n",
    "$E[X] = $ the expectation of $X$.  \n",
    "$I(X) = $ the information content of $X$.   \n",
    "$P(X) = $ probability of $X$.  \n",
    "$b = $ base of the logarithm.    \n",
    "\n",
    "This rather abstract formula gives us a way to compute the expected information content of a set of values $X$. The more likely (higher probability) of $X$ the less informative it is. \n",
    "\n",
    "To create a loss function from the definition of Shannon entropy we start with the **Kullback-Leibler divergence (KL divergence)** or **relative entropy**. The KL divergence is an information theoretic measure of the difference between two distributions, $P(X)$ and $Q(X)$.\n",
    "\n",
    "$$\\mathbb{D}_{KL}(P \\parallel Q) = - \\sum_{i=1}^n p(x_i)\\ ln_b \\frac{p(x_i)}{q(x_i)}$$\n",
    "\n",
    "Ideally, in the case of training a machine learning model we want a distribution $Q(X)$, which is identical to the actual data distribution $P(X)$. \n",
    "\n",
    "But, you may say, if we could know $P(X)$ why compute $Q(X)$ at all? Fortunately, we do not have to. We can rewrite the KL divergence as:\n",
    "\n",
    "$$\\mathbb{D}_{KL}(P \\parallel Q) = \\sum_{i=1}^n p(x_i)\\ ln_b p(x_i) - \\sum_{i=1}^n p(x_i)\\ ln_b q(x_i)$$\n",
    "\n",
    "Since $P(X)$ is fixed and we wish to find $Q(X)$ when we train our model, we can minimize the term on the right, which is the **cross entropy** defined as:\n",
    "\n",
    "$$\\mathbb{H}(P,Q) = - \\sum_{i=1}^n p(x_i)\\ ln_b q(x_i)$$\n",
    "\n",
    "From the formulation of KL divergence above you can see the following.\n",
    "\n",
    "$$\\mathbb{D}_{KL}(P \\parallel Q) = \\mathbb{H}(P) + \\mathbb{H}(P,Q)\\\\\n",
    "\\mathbb{D}_{KL}(P \\parallel Q) = Entropy(P) + Cross\\ Entropy(P,Q)$$\n",
    "\n",
    "Thus, we can minimize divergence by minimizing cross entropy. This idea is both intuitive and computationally attractive. The closer the estimated distribution $q(X)$ is to the distribution of the true underling process $p(X)$, the lower the cross entropy and the lower the KL divergence. \n",
    "\n",
    "In general we will not know $p(X)$. In fact, if we did, why would we need to solve a training problem? So, we can use the following approximation.\n",
    "\n",
    "$$\\mathbb{H}(P,Q) = - \\frac{1}{N} \\sum_{i=1}^n ln_b q(x_i)$$\n",
    "\n",
    "You may notice, that this approximation, using the average log likelihood, is equivalent to a maximum likelihood estimator (MLE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a specific case of a model with Gaussian likelihood. What is the cross entropy? We can start by thinking about the definition of likelihood. \n",
    "\n",
    "$$p(data|model) = p(data|f(\\theta)) = p(x_i|f(\\hat{\\mu},\\sigma))= \\frac{1}{2 \\pi \\sigma^2} e^{\\frac{-(x_i - \\hat{\\mu})^2}{2 \\sigma^2}}$$\n",
    "\n",
    "We take the negative logarithm of this likelihood model. \n",
    "\n",
    "$$-log\\big(p(data|model) \\big) = - \\frac{1}{2}\\big( log( 2 \\pi \\sigma^2) + \\frac{(x_i - \\hat{\\mu})^2}{2 \\sigma^2} \\big)$$\n",
    "\n",
    "Now, the first term on the right is a constant, as is the denominator of the second term if we assume known variance. Since our goal is to minimize cross entropy, we can eliminate these quantities and be left with just the following.\n",
    "\n",
    "$$-(x_i - \\hat{\\mu})^2$$\n",
    "\n",
    "This is one issue we need to deal with. Our formulation of cross entropy involves the unknown true distribution of the underling process $p(X)$. However, since $p(x_i)$ is fixed but unknown we can just write the following.\n",
    "\n",
    "$$min \\big( \\mathbb{H}(P,Q) \\big) \\propto argmin_{\\mu} \\big( - \\sum_{i=1}^n (x_i - \\hat{\\mu})^2 \\big)$$\n",
    "\n",
    "This is just the definition of a Maximum Likelihood Estimator (MLE) for the least squares problem! In fact, since the cross entropy is computed using the negative log likelihood, it will always be minimized by the MLE. \n",
    "\n",
    "You can see another example of [cross-entropy error function and logistic regression](https://en.wikipedia.org/wiki/Cross_entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 Computing Loss functions\n",
    "\n",
    "The loss function is used to train the model. Therefore the loss functon must be computed in an efficient manner. \n",
    "\n",
    "Given the number of parameters in deep neural nets over-fitting is inevitable. Therefore some regularization is required. We will discuss regularization in greater depth in another lesson. For now, we will just use the following regularized form.\n",
    "\n",
    "$$\\mathbb{H}(P,Q) = J(\\theta) = - \\frac{1}{N}\\sum_{i=1}^n ln_b q(x_i|\\theta) + \\lambda ||\\theta||^2\\\\ \n",
    "where\\\\\n",
    "- \\frac{1}{N}\\sum_{i=1}^n ln_b q(x_i|\\theta) = J_{MLE}(\\theta)\\\\\n",
    "||\\theta||^2 = L^2\\ norm\\ regularization\\ term$$\n",
    "\n",
    "To minimize $J(\\theta)$ in this form $\\theta$ must be chosen to keep $||\\theta||^2$ small while minimizing the negative log likelihood of $q(x_i|\\theta)$.\n",
    "\n",
    "Let's  consider how we would compute this form of the lost function. The compuational graph shown below illustrates the compuational path for the reqularized loss function. For simplicity, no bias terms are considered.\n",
    "\n",
    "<img src=\"Figures/CompGraph1.jpg\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
    "<center>**Figure 3.1  \n",
    "Computional graph for computing loss of fully connected neural network of Figure 2.3** </center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Chain rule of calculus\n",
    "\n",
    "Key to the back propagation algorithm is the chain rule of calculus; not to be confused with the chain rule of probability. The chain rule allows us to back propagate gradients though an arbitrarily complex graph of functions. \n",
    "\n",
    "Now, suppose there is a function $y = g(x)$, and another function $z = f(y) = f(g(x))$. How do we compute the derivative of $z$ with respect to $x$? Applying the chain rule we get: \n",
    "\n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy}\\frac{dy}{dx}$$\n",
    "\n",
    "Consider $x \\in R^M$ $g(x) \\Rightarrow R^M$ and $ f(y) \\Rightarrow z \\in R$. The chain rule becomes:\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial x} = \\sum_{j \\in M} \\frac{\\partial z}{\\partial y_j}\\frac{\\partial y_j}{\\partial x_i}$$\n",
    "\n",
    "Which we can rewrite as  \n",
    "\n",
    "$$\\nabla_{x}z = \\Big( \\frac{\\partial x}{\\partial y} \\Big)^T \\nabla_{y}z$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $\\frac{\\partial x}{\\partial y}$ is the $n x m$ **Jacobian matrix** of partial derivatives. The Jacobian is multiplied by the gradient with respect to $y$, $\\nabla_{y}z$. You can think of the Jacobian as a transformation for a gradient with respect to $y$ to what we really want, the gradient with respect to $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Example of finding a gradient. \n",
    "\n",
    "Let's work out backpropagation for a very simple neural network with a just an input layer and an output layer. This neural network, including the loss function, is shown in Figure 3.2 below. This network has been highly simplified. There are only three layers, input layer, a two unit hidden layer with no bias terms, and a single unit output layer. There are only two weight tensors for this network. Further, the hidden units use rectilinear activation and the output unit uses linear activation. These activation functions have simple partial derivatives.  \n",
    "\n",
    "<img src=\"Figures/LossGraph.jpg\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
    "<center>**Figure 3.2 \n",
    "Simple single layer neural network with loss function** </center>\n",
    "\n",
    "To analyze this network we will refer to the computational graph shown in Figure 3.1 above. \n",
    "\n",
    "First, we need to work out the forward propagation relationships. We can compute the outputs of the hidden layer as follows.\n",
    "\n",
    "$$S_{\\{1,2\\}} = \\sigma_h \\big( W^1 \\cdot X_{\\{1,2\\}} \\big) = \\sigma \\big( \\sum_j W^1_{i,j} x_j \\big)$$  \n",
    "\n",
    "In the same way, the result from the output layer can be computed as follows, since the activation function for this layer is linear. \n",
    "\n",
    "$$S_3 = W^2 \\cdot S_{\\{1,2\\}} = \\sum_i W^2_i \\sigma \\big( \\sum_j W^1_{i,j} x_j \\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform backpropagation, we need fill out the gradient vector by computing $\\frac{\\partial J(W)}{\\partial W}$ for each weight in the model. \n",
    "\n",
    "$$\\frac{\\partial J(W)}{\\partial W} = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial J(W)}{\\partial W^2_{11}} \\\\\n",
    "\\frac{\\partial J(W)}{\\partial W^2_{12}} \\\\\n",
    "\\frac{\\partial J(W)}{\\partial W^2_{21}} \\\\\n",
    "\\frac{\\partial J(W)}{\\partial W^2_{22}} \\\\\n",
    "\\frac{\\partial J(W)}{\\partial W^1_{1}} \\\\\n",
    "\\frac{\\partial J(W)}{\\partial W^1_{2}}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple in this example we will just use a non-normalized squared error loss function. This is just the MLE estimator (without normalization) for a Gaussian distribution. \n",
    "\n",
    "$$J(W) = - \\frac{1}{2} \\sum_{l=1}^n (y_l - S_{3,l})^2 $$\n",
    "\n",
    "Where:  \n",
    "$y_k = $ the label for the lth case.     \n",
    "$\\hat{y_k} = S_{3,k} =$ the output of the network for the lth case. \n",
    "\n",
    "We want to compute the gradients with respect to the input and output tensors:\n",
    "\n",
    "$$\\frac{\\partial J(W)}{\\partial W^1}, \\ \\frac{\\partial J(W)}{\\partial W^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the easier case of the partial derivatives with respect to the output tensor. We can apply the chain rule as follows:\n",
    "\n",
    "$$\\frac{\\partial J(W)}{\\partial W^2_k} = \\frac{\\partial  J(W)}{\\partial S_{3,k}} \\frac{\\partial S_{3,k}}{\\partial W^2_k}$$\n",
    "\n",
    "The first partial derivative of the chain is:\n",
    "\n",
    "$$\\frac{\\partial J(W)}{\\partial S_{3,k}} = \\frac{\\partial - \\frac{1}{2} (y_k - S_{3,k})^2} {\\partial S_{3,k}} = y_k - S_{3,k} $$\n",
    "\n",
    "And, the partial derivative of the second partial derivative in the chain, given the linear activation of the output unit:\n",
    "\n",
    "$$\\frac{\\partial S_{3,k}}{\\partial W^2_k} = \\frac{\\partial W^2_k S_{j,k}}{\\partial W^2_k}  = S_{j,l}, \\ j \\in \\{1,2\\}$$\n",
    "\n",
    "Multiplying the two components of the chain gives us:\n",
    "\n",
    "$$\\frac{\\partial J(W)}{\\partial W^2_k} = S_{j,k} (y_k - S_{3,k}), \\ j \\in \\{1,2\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivatives with respect to the input tensor are a bit more complicated. To apply the chain rule we must work backwards from the loss function. This gives the following chain:\n",
    "\n",
    "$$\\frac{\\partial J(W)}{\\partial W^1_{i,j}} =  \\frac{\\partial J(W)}{\\partial S_{3}} \\frac{\\partial S_{3}}{\\partial S_{j}} \\frac{\\partial S_{j}}{\\partial W^1_{i,j}}$$\n",
    "\n",
    "First, we find the right most partial derivative in our chain:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial S_j}{\\partial W^1_{i,j}} = \n",
    "\\begin{cases}\n",
    "     \\frac {\\partial W^1_{i,j} x_{i,k}}{\\partial W^1_{i,j}}, & \\text{if $S_j>0$} \\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Which given the ReLU activation results in:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial S_j}{\\partial W^1_{i,j}} = \n",
    "\\begin{cases}\n",
    "    1, & \\text{if $S_j>0$}  \\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The middle partial derivative must account for the nonlinearity:\n",
    "\n",
    "$$\\frac{\\partial S_{3}}{\\partial S_{j}} = W^2_j$$\n",
    "\n",
    "We have already computed $\\frac{\\partial J(W)}{\\partial S_{3}}$. Multiplying all three partial derivatives we find:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(W)}{\\partial W^1_{i,j}} = \n",
    "\\begin{cases}\n",
    "    (y_k - S_{3,k}) W^2_j, & \\text{if $S_j>0$} \\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Where $S_3$ and $S_{\\{1,2 \\}} are computed using the relationships given above. \n",
    "\n",
    "A more detailed, but still digestable example of computing gradients for backpropagation can be found in a blog post by [Manfred Zaharauskas](http://blog.manfredas.com/backpropagation-tutorial/), among many other places. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.0 Performance Metrics\n",
    "\n",
    "Now that we have the components for traning a basic neural network in place we need a way to evaluate the performance. It turns out, there is nothing special evaluation of neural network models as opposed to other machine learning models. \n",
    "\n",
    "For regression models, one typically use the standard metrics such as root mean square error (RMSE), mean absolute error (MAE). \n",
    "\n",
    "For classification models, one also typically uses the standard metrics including the confusion matrix, accuracy, precision and recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Copyright 2018, 2019, Stephen F Elston. All rights reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
